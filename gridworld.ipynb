{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym \n",
    "import gym_gridworlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "[[ 0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1 -1.   0.1]\n",
      " [ 0.1  0.1  0.1  0.1  1. ]]\n",
      "-------------\n",
      "valores\n",
      "[[4.79189612 5.22236333 5.70066023 6.23210123 6.82259123]\n",
      " [5.22236333 5.70066023 6.23210123 6.82259123 7.47869123]\n",
      " [5.70066023 6.23210123 6.82259123 7.47869123 8.20769123]\n",
      " [6.23210123 6.82259123 7.47869123 7.10769123 9.01769123]\n",
      " [6.82259123 7.47869123 8.20769123 9.01769123 9.91769123]]\n",
      "-------------\n",
      "acao da politica\n",
      "['right', 'right', 'right', 'right', 'down']\n",
      "['down', 'right', 'right', 'right', 'down']\n",
      "['right', 'right', 'right', 'right', 'down']\n",
      "['right', 'right', 'down', 'right', 'down']\n",
      "['right', 'right', 'right', 'right', 'right']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# global variables\n",
    "BOARD_ROWS = 5\n",
    "BOARD_COLS = 5\n",
    "WIN_STATE = (4, 4)\n",
    "LOSE_STATE = (3, 3)\n",
    "START = (0, 0)\n",
    "DETERMINISTIC = True\n",
    "WIN_REWARD = 1\n",
    "LOSE_REWARD = -1\n",
    "TRANSITION_REWARD = -0.05\n",
    "CONVERGENCE_FACTOR = 0.01\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "class GridWorld:\n",
    "    def __init__(self, state=START):\n",
    "        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
    "        self.board[1, 1] = -1\n",
    "        self.state = state\n",
    "        self.isEnd = False\n",
    "        self.determine = DETERMINISTIC\n",
    "        \n",
    "    def giveReward(self):\n",
    "        if self.state == WIN_STATE:\n",
    "            return WIN_REWARD\n",
    "        elif self.state == LOSE_STATE:\n",
    "            return LOSE_REWARD\n",
    "        else:\n",
    "            return TRANSITION_REWARD\n",
    "\n",
    "    def get_reward_table(self):\n",
    "        # Inicializa uma tabela de recompensas com zeros para todos os estados\n",
    "        reward_table = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
    "        # Percorre todas as linhas e colunas do grid dando as recompensas\n",
    "        for r in range(BOARD_ROWS):\n",
    "            for c in range(BOARD_COLS):\n",
    "                state_tuple = (r,c)\n",
    "                if state_tuple == WIN_STATE:\n",
    "                    reward_table[r][c] = WIN_REWARD\n",
    "                elif state_tuple == LOSE_STATE:\n",
    "                    reward_table[r][c] = LOSE_REWARD\n",
    "                else:\n",
    "                    reward_table[r][c] = TRANSITION_REWARD\n",
    "        return reward_table\n",
    "\n",
    "    def isEndFunc(self):\n",
    "        if (self.state == WIN_STATE) or (self.state == LOSE_STATE):\n",
    "            self.isEnd = True\n",
    "\n",
    "    def nxtPosition(self,state, action):\n",
    "        \"\"\"\n",
    "        action: up, down, left, right\n",
    "        -------------\n",
    "        0 | 1 | 2| 3|\n",
    "        1 |\n",
    "        2 |\n",
    "        return next position\n",
    "        \"\"\"\n",
    "        if self.determine:\n",
    "            if action == \"up\":\n",
    "                nxtState = (state[0] - 1, state[1])\n",
    "            elif action == \"down\":\n",
    "                nxtState = (state[0] + 1, state[1])\n",
    "            elif action == \"left\":\n",
    "                nxtState = (state[0], state[1] - 1)\n",
    "            else:\n",
    "                nxtState = (state[0], state[1] + 1)\n",
    "            # if next state legal\n",
    "            if (nxtState[0] >= 0) and (nxtState[0] <= (BOARD_ROWS -1)):\n",
    "                if (nxtState[1] >= 0) and (nxtState[1] <= (BOARD_COLS -1)):\n",
    "                    if nxtState != (1, 1):\n",
    "                        return nxtState\n",
    "            return state\n",
    "\n",
    "    def showBoard(self):\n",
    "        self.board[self.state] = 1\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-----------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = '*'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'z'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = '0'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-----------------')\n",
    "\n",
    "\n",
    "# Agent of player\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.State = GridWorld()\n",
    "        self.lr = 0.2\n",
    "        self.exp_rate = 0.3\n",
    "        # initial state reward\n",
    "        self.state_values = {}\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                self.state_values[(i, j)] = 0  # set initial value to 0\n",
    "\n",
    "    def takeAction(self, action):\n",
    "        position = self.State.nxtPosition(GridWorld.state,action)\n",
    "        return GridWorld(state=position)\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.State = GridWorld()\n",
    "\n",
    "    def optimizeMDP(self, reward_table):\n",
    "        values = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
    "        policy = [[\"\" for _ in range(BOARD_COLS)] for _ in range(BOARD_ROWS)]\n",
    "        converge = False\n",
    "        while not converge:\n",
    "            delta = 0\n",
    "            for r in range(BOARD_ROWS):\n",
    "                for c in range(BOARD_COLS):\n",
    "                    temp = values[r][c]\n",
    "                    max_value = float('-inf')\n",
    "\n",
    "                    for a in self.actions:\n",
    "                        next_row , next_col = self.State.nxtPosition((r,c), a)\n",
    "                        max_value = max(max_value,values[next_row][next_col])\n",
    "                        if(values[next_row][next_col] == max_value):\n",
    "                            policy[r][c] = a \n",
    "\n",
    "                    values[r][c] = reward_table[r][c] + DISCOUNT_FACTOR*max_value\n",
    "                    delta = max(delta, abs(temp - values[r][c]))\n",
    "                if delta < CONVERGENCE_FACTOR:\n",
    "                    converge = True\n",
    "            self.state_values = values\n",
    "        return values , policy                 \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    grid = GridWorld()\n",
    "    agente = Agent()\n",
    "    table = grid.get_reward_table()\n",
    "    print('rewards')\n",
    "    print( table)\n",
    "    print('-------------')\n",
    "    print('valores')\n",
    "    print(agente.optimizeMDP(table)[0])\n",
    "    print('-------------')\n",
    "    print('acao da politica')\n",
    "    for i in range(BOARD_ROWS):\n",
    "        print(agente.optimizeMDP(table)[1][i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "[[ 0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1 -1.   0.1]\n",
      " [ 0.1  0.1  0.1  0.1  1. ]]\n",
      "-------------\n",
      "valores\n",
      "[[ 0.1984375   0.19921875  0.19960938  0.19980469  0.19990234]\n",
      " [ 0.19921875  0.19960938  0.19980469  0.19990234  0.31245117]\n",
      " [ 0.19960938  0.19980469  0.19990234  0.31245117  0.53745117]\n",
      " [ 0.19980469  0.19990234  0.31245117 -0.56254883  0.98745117]\n",
      " [ 0.19990234  0.31245117  0.53745117  0.98745117  1.88745117]]\n",
      "-------------\n",
      "acao da politica\n",
      "['right', 'right', 'right', 'right', 'left']\n",
      "['down', 'right', 'right', 'right', 'down']\n",
      "['right', 'right', 'right', 'right', 'down']\n",
      "['right', 'right', 'down', 'right', 'down']\n",
      "['right', 'right', 'right', 'right', 'right']\n"
     ]
    }
   ],
   "source": [
    "BOARD_ROWS = 5\n",
    "BOARD_COLS = 5\n",
    "WIN_STATE = (4, 4)\n",
    "LOSE_STATE = (3, 3)\n",
    "START = (0, 0)\n",
    "DETERMINISTIC = True\n",
    "WIN_REWARD = 1\n",
    "LOSE_REWARD = -1\n",
    "TRANSITION_REWARD = 0.1\n",
    "CONVERGENCE_FACTOR = 0.01\n",
    "DISCOUNT_FACTOR = 0.5\n",
    "if __name__ == \"__main__\":\n",
    "    grid = GridWorld()\n",
    "    agente = Agent()\n",
    "    table = grid.get_reward_table()\n",
    "    print('rewards')\n",
    "    print( table)\n",
    "    print('-------------')\n",
    "    print('valores')\n",
    "    print(agente.optimizeMDP(table)[0])\n",
    "    print('-------------')\n",
    "    print('acao da politica')\n",
    "    for i in range(BOARD_ROWS):\n",
    "        print(agente.optimizeMDP(table)[1][i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "[[ 0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1 -1.   0.1]\n",
      " [ 0.1  0.1  0.1  0.1  1. ]]\n",
      "-------------\n",
      "valores\n",
      "[[ 0.27416108  0.29935532  0.34134572  0.41132972  0.52796972]\n",
      " [ 0.29935532  0.34134572  0.41132972  0.52796972  0.72236972]\n",
      " [ 0.34134572  0.41132972  0.52796972  0.72236972  1.04636972]\n",
      " [ 0.41132972  0.52796972  0.72236972 -0.05363028  1.58636972]\n",
      " [ 0.52796972  0.72236972  1.04636972  1.58636972  2.48636972]]\n",
      "-------------\n",
      "acao da politica\n",
      "['right', 'right', 'right', 'right', 'down']\n",
      "['down', 'right', 'right', 'right', 'down']\n",
      "['right', 'right', 'right', 'right', 'down']\n",
      "['right', 'right', 'down', 'right', 'down']\n",
      "['right', 'right', 'right', 'right', 'right']\n"
     ]
    }
   ],
   "source": [
    "BOARD_ROWS = 5\n",
    "BOARD_COLS = 5\n",
    "WIN_STATE = (4, 4)\n",
    "LOSE_STATE = (3, 3)\n",
    "START = (0, 0)\n",
    "DETERMINISTIC = True\n",
    "WIN_REWARD = 1\n",
    "LOSE_REWARD = -1\n",
    "TRANSITION_REWARD = 0.1\n",
    "CONVERGENCE_FACTOR = 0.01\n",
    "DISCOUNT_FACTOR = 0.6\n",
    "if __name__ == \"__main__\":\n",
    "    grid = GridWorld()\n",
    "    agente = Agent()\n",
    "    table = grid.get_reward_table()\n",
    "    print('rewards')\n",
    "    print( table)\n",
    "    print('-------------')\n",
    "    print('valores')\n",
    "    print(agente.optimizeMDP(table)[0])\n",
    "    print('-------------')\n",
    "    print('acao da politica')\n",
    "    for i in range(BOARD_ROWS):\n",
    "        print(agente.optimizeMDP(table)[1][i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "achei interessante esse pq se vc coloca pra ele ter fator de desconto 1 e valor de transicao maior q 0 ele ve muito no futuro e acaba rodando pra sempre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "[[ 0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1 -1.   0.1]\n",
      " [ 0.1  0.1  0.1  0.1  1. ]]\n",
      "-------------\n",
      "valores\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\guilh\\AppData\\Local\\Temp\\ipykernel_24220\\969363149.py\", line 20, in <module>\n",
      "    print(agente.optimizeMDP(table)[0])\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\guilh\\AppData\\Local\\Temp\\ipykernel_24220\\1804627987.py\", line None, in optimizeMDP\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"c:\\Users\\guilh\\anaconda3\\envs\\columbia_ai\\Lib\\site-packages\\executing\\executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BOARD_ROWS = 5\n",
    "BOARD_COLS = 5\n",
    "WIN_STATE = (4, 4)\n",
    "LOSE_STATE = (3, 3)\n",
    "START = (0, 0)\n",
    "DETERMINISTIC = True\n",
    "WIN_REWARD = 1\n",
    "LOSE_REWARD = -1\n",
    "TRANSITION_REWARD = 0.1\n",
    "CONVERGENCE_FACTOR = 0.01\n",
    "DISCOUNT_FACTOR = 1\n",
    "if __name__ == \"__main__\":\n",
    "    grid = GridWorld()\n",
    "    agente = Agent()\n",
    "    table = grid.get_reward_table()\n",
    "    print('rewards')\n",
    "    print( table)\n",
    "    print('-------------')\n",
    "    print('valores')\n",
    "    print(agente.optimizeMDP(table)[0])\n",
    "    print('-------------')\n",
    "    print('acao da politica')\n",
    "    for i in range(BOARD_ROWS):\n",
    "        print(agente.optimizeMDP(table)[1][i] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "columbia_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
